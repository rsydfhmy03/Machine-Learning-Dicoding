# -*- coding: utf-8 -*-
"""SubmissionDicoding_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qQq6U5dBLsseD3Dlk_EVX5Kf5kOh5dc3

# __DICODING MACHINE LEARNING__
Nama : Fahmy Rosyadi <br>
Asal Instansi : Politeknik Negeri Jember
<br>
Proyek : Sentiment Analisis Ulasan Debat Pemilu 2024 Menggunakan NLP

## Load Dataset CSV dari Google Drive
"""

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/DatasetCollab/UlasanDebat/UlasanDebatPemilu24.csv")
df.head(10)

df = df[['full_text', 'username', 'Kategori','created_at']]
df

df.shape

"""## Lakukan Preprocessing Data"""

df = df.drop_duplicates(subset=['full_text'])

df.duplicated().sum()

df = df.dropna()

df.isnull().sum()

df.shape

def clean_twitter_text(text):
  text = re.sub(r'@[A-Za-z0-9+]',' ', text)
  text = re.sub(r'#\w+',' ', text)
  text = re.sub(r'RT[\s]+',' ',text)
  text = re.sub(r'https?://\S+',' ', text)

  text = re.sub(r'[^A-Za-z0-9]',' ',text)
  text = re.sub(r'\s+', ' ', text).strip()

  return text

df['full_text'] = df['full_text'].apply(clean_twitter_text)

df['full_text'] = df['full_text'].str.lower()

df

norm = {"yg": "yang",'nggak': 'tidak', 'akaaalllll':'akal','msk':'masuk',"nipu":"menipu","tdk" : "tidak",'bangetdari': ' banget dari', "vibes" : "suasana","menilau" : "menilai", "mo" : "ingin"}

def normalisasi(str_text):
  for i in norm:
    str_text = str_text.replace(i, norm[i])
  return str_text

df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))
df

!pip install Sastrawi

#Stop Words
import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
more_stop_words = ["tidak","yang","dan","di"]

stop_words = StopWordRemoverFactory().get_stop_words()
stop_words.extend(more_stop_words)

new_array = ArrayDictionary(stop_words)
new_stop_words_remover = StopWordRemover(new_array)

def stopword(str_text):
  str_text = new_stop_words_remover.remove(str_text)
  return str_text

df['full_text'] = df['full_text'].apply(lambda x: stopword(x))
df.head()

import googletrans as gt
from googletrans import Translator

def convert_eng(tweet):
    translator = Translator()
    translation = translator.translate(tweet, src='id', dest='en')
    return translation.text

df['tweet_english'] = df['full_text'].apply(convert_eng)
df.to_csv('/content/drive/MyDrive/DatasetCollab/UlasanDebat/TranslateUlasanDebatPemilu.csv')

data = pd.read_csv('/content/drive/MyDrive/DatasetCollab/UlasanDebat/TranslateUlasanDebatPemilu.csv')
data.head(500)

!pip install tweet-preprocessor
!pip install textblob
!pip install wordcloud
!pip install nltk

import preprocessor as pre
from textblob import TextBlob
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

nltk.download('punkt')

data_tweet = list(data['tweet_english'])
polaritas = 0

status = []
total_positif = total_negatif = total_netral = total = 0

for i, tweet in enumerate(data_tweet):
    analysis = TextBlob(tweet)
    polaritas += analysis.polarity

    if analysis.sentiment.polarity > 0.0:
      total_positif += 1
      status.append('Positif')
    elif analysis.sentiment.polarity == 0.0:
      total_netral += 1
      status.append('Netral')
    else:
      total_negatif += 1
      status.append('Negatif')
    total +=1

print(f'Hasil Analisis Data:\nPositif = {total_positif}\nNetral = {total_netral}\nNegatif = {total_negatif}')
print(f'Total Data : {total}')

data['klasifikasi'] = status
data

data.info()

data.klasifikasi.value_counts()

from wordcloud import WordCloud, STOPWORDS

def plot_cloud(wordcloud):
  plt.figure(figsize=(10,8))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.show()

all_words = ' '.join([tweet for tweet in data['full_text']])

wordcloud = WordCloud(
    width = 2800,
    height = 1800,
    random_state=3,
    background_color='black',
    colormap = 'Blues_r',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)

plot_cloud(wordcloud)

"""## Pelabelan data hasil Preprocessing"""

from keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization
from keras.models import Model
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

nltk.download('wordnet')
nltk.download('stopwords')

# klasifikasi to one-hot-encoding
klasifikasi = pd.get_dummies(data.klasifikasi)
df_new = pd.concat([data, klasifikasi], axis=1)
df_new = df_new.drop(columns='klasifikasi')
df_new.head(10)

# change dataframe value to numpy array
text = df_new['full_text'].values
label = df_new[['Negatif', 	'Netral', 'Positif']].values

label.shape

"""## Split data train dan testing"""

# Split data into training and validation
from sklearn.model_selection import train_test_split
text_train, text_test, label_train, label_test = train_test_split(text, label, test_size=0.2, shuffle=True)

label_train

"""## Pelatihan Model"""

# tokenizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_test)

sekuens_train = tokenizer.texts_to_sequences(text_train)
sekuens_test = tokenizer.texts_to_sequences(text_test)

padded_train = pad_sequences(sekuens_train)
padded_test = pad_sequences(sekuens_test)

import tensorflow as tf
from keras.losses import CategoricalCrossentropy
from keras.optimizers import Adam

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5001, output_dim=100, input_length=None),
    tf.keras.layers.LSTM(100),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001),metrics=['acc'], loss=CategoricalCrossentropy(),run_eagerly=True)
model.summary()

# callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
      accuracy = logs.get('acc')
      val_accuracy = logs.get('val_accuracy')
      if accuracy is not None and accuracy == 1.00:
        print("\nAkurasi telah mencapai 100%!")
        self.model.stop_training = True

callbacks = myCallback()

history = model.fit(padded_train, label_train, epochs=50,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks], validation_steps=30)

"""## Evaluasi Model"""

plt.plot(history.history['acc'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend( loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend( loc='upper left')
plt.show()

"""## Testing Model"""

import numpy as np

class_labels = ['Negatif', 'Netral', 'Positif']
label_predict_prob = model.predict(padded_test)
predicted_classes = np.argmax(label_predict_prob, axis=1)

threshold_positif = 0.5
threshold_netral = 0.5

predicted_labels = []

for prob in label_predict_prob:
    if prob[2] >= threshold_positif:
        predicted_labels.append('Positif')
    elif prob[1] >= threshold_netral:
        predicted_labels.append('Netral')
    else:
        predicted_labels.append('Negatif')

df_results = pd.DataFrame({
    'Text': text_test,
    'True_Label': [class_labels[np.argmax(label)] for label in label_test],
    'Predicted_Label': predicted_labels,
    'Probability': [max(prob) for prob in label_predict_prob]
})

df_results.head(15)