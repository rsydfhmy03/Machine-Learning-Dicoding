# -*- coding: utf-8 -*-
"""SubmissionDicoding_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qQq6U5dBLsseD3Dlk_EVX5Kf5kOh5dc3

# __DICODING MACHINE LEARNING__
Nama : Fahmy Rosyadi <br>
Asal Instansi : Politeknik Negeri Jember
<br>
Proyek : Sentiment Analisis Ulasan Debat Pemilu 2024 Menggunakan NLP

## Load Dataset CSV dari Google Drive

> Import All Libraries
"""

!pip install Sastrawi

!pip install googletrans==4.0.0-rc1

!pip install preprocessor

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
import googletrans as gt
from googletrans import Translator
import preprocessor as pre
from textblob import TextBlob
import nltk
import numpy as np
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud, STOPWORDS
from keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization
from keras.models import Model
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import tensorflow as tf
from keras.losses import CategoricalCrossentropy
from keras.optimizers import Adam
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

df = pd.read_csv("/content/drive/MyDrive/DatasetCollab/UlasanDebat/UlasanDebatPemilu24.csv")
df.head(10)

df = df[['full_text', 'username', 'Kategori','created_at']]
df

df.shape

"""## Lakukan Preprocessing Data"""

df = df.drop_duplicates(subset=['full_text'])

df.duplicated().sum()

df = df.dropna()

df.isnull().sum()

df.shape

def clean_twitter_text(text):
    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text)
    text = re.sub(r'#\w+', ' ', text)
    text = re.sub(r'RT[\s]+', ' ', text)
    text = re.sub(r'https?://\S+', ' ', text)

    text = re.sub(r'[^A-Za-z0-9 ]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

df['full_text'] = df['full_text'].apply(clean_twitter_text)

df['full_text'] = df['full_text'].str.lower()

df

norm = {"yg": "yang",'nggak': 'tidak', 'akaaalllll':'akal','msk':'masuk',"nipu":"menipu","tdk" : "tidak",'bangetdari': ' banget dari', "vibes" : "suasana","menilau" : "menilai", "mo" : "ingin"}

def normalisasi(str_text):
  for i in norm:
    str_text = str_text.replace(i, norm[i])
  return str_text

df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))
df

more_stop_words = ["tidak","yang","dan","di","g"]

stop_words = StopWordRemoverFactory().get_stop_words()
stop_words.extend(more_stop_words)

new_array = ArrayDictionary(stop_words)
new_stop_words_remover = StopWordRemover(new_array)

def stopword(str_text):
  str_text = new_stop_words_remover.remove(str_text)
  return str_text

df['full_text'] = df['full_text'].apply(lambda x: stopword(x))
df.head()

def convert_eng(tweet):
    translator = Translator()
    translation = translator.translate(tweet, src='id', dest='en')

    if translation and hasattr(translation, 'text'):
        return translation.text
    else:
        return tweet

df['tweet_english'] = df['full_text'].apply(convert_eng)
df.to_csv('/content/drive/MyDrive/DatasetCollab/UlasanDebat/TranslateUlasanDebatPemilu.csv')

data = pd.read_csv('/content/drive/MyDrive/DatasetCollab/UlasanDebat/TranslateUlasanDebatPemilu.csv')
data.head(500)

data_tweet = list(data['tweet_english'])
polaritas = 0

status = []
total_positif = total_negatif = total_netral = total = 0

for i, tweet in enumerate(data_tweet):
    analysis = TextBlob(tweet)
    polaritas += analysis.polarity

    if analysis.sentiment.polarity > 0.0:
      total_positif += 1
      status.append('Positif')
    elif analysis.sentiment.polarity == 0.0:
      total_netral += 1
      status.append('Netral')
    else:
      total_negatif += 1
      status.append('Negatif')
    total +=1

print(f'Hasil Analisis Data:\nPositif = {total_positif}\nNetral = {total_netral}\nNegatif = {total_negatif}')
print(f'Total Data : {total}')

data['klasifikasi'] = status
data

data.info()

data.klasifikasi.value_counts()

def plot_cloud(wordcloud):
  plt.figure(figsize=(10,8))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.show()

all_words = ' '.join([tweet for tweet in data['full_text']])

wordcloud = WordCloud(
    width = 2800,
    height = 1800,
    random_state=3,
    background_color='black',
    colormap = 'Blues_r',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)

plot_cloud(wordcloud)

all_words = ' '.join([tweet for tweet in data['tweet_english']])

wordcloud = WordCloud(
    width = 2800,
    height = 1800,
    random_state=3,
    background_color='black',
    colormap = 'Blues_r',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)

plot_cloud(wordcloud)

"""## Pelabelan data"""

klasifikasi = pd.get_dummies(data.klasifikasi)
df_new = pd.concat([data, klasifikasi], axis=1)
df_new = df_new.drop(columns='klasifikasi')
df_new.head(10)

def remove_stopwords(text):
  words = text.split()
  new_text = ""
  for word in words:
    if word not in stopwords.words('english'):
      new_text += word
      new_text += " "

  return new_text

def lematized_text(text):
  lematizer = WordNetLemmatizer()
  words = text.split()
  new_text = ""
  for word in words:
    lematized_word = lematizer.lemmatize(word)
    new_text += lematized_word
    new_text += " "
  return new_text


def preprocessing (text):
  text = text.lower()
  text = remove_stopwords(text)
  text = lematized_text(text)

  return text

for i in range(len(df_new)):
    df_new.loc[i, 'tweet_english'] = preprocessing(df_new.loc[i, 'tweet_english'])

text = df_new['tweet_english'].values
label = df_new[['Negatif', 	'Netral', 'Positif']].values

df_new['tweet_english']

"""## Split data train dan testing"""

from sklearn.model_selection import train_test_split
text_train, text_test, label_train, label_test = train_test_split(text, label, test_size=0.2, shuffle=True)

label_train

"""## Pelatihan Model"""

def get_avg(text_list):
  sum = 0
  for text in text_list:
    word_num = len(text)
    sum = sum + word_num
  return sum/len(text_list)

tokenizer = Tokenizer(num_words=1000, oov_token='<oov>')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_test)

sequences_train = tokenizer.texts_to_sequences(text_train)
sequences_test = tokenizer.texts_to_sequences(text_test)

max_len_train = int(get_avg(text_train))
max_len_test = int(get_avg(text_test))
max_len = max(max_len_train, max_len_test)
print(max_len_train, " : ", max_len_test," : ", max_len)
padded_train = pad_sequences(sequences_train, maxlen=max_len, padding='post', truncating='post')
padded_test = pad_sequences(sequences_test, maxlen=max_len, padding='post', truncating='post')

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=1001, output_dim=550, input_length=89),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])

val_accuracy = tf.keras.metrics.Accuracy(name='val_accuracy')
model.compile(optimizer= 'adam', metrics=['accuracy'], loss= CategoricalCrossentropy())
model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
      accuracy = logs.get('accuracy')
      val_accuracy = logs.get('val_accuracy')
      if accuracy is not None and accuracy > 0.90 and val_accuracy > 0.90:
        print("\nAkurasi telah mencapai > 90%!")
        self.model.stop_training = True

callbacks = myCallback()

len(label)

history = model.fit(padded_train,
                    label_train,
                    epochs=50,
                    steps_per_epoch=50,
                    validation_data=(padded_test, label_test),
                    verbose=2,
                    callbacks=[callbacks],
                    validation_steps=25)

"""## Evaluasi Model"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Testing Model"""

class_labels = ['Negatif', 'Netral', 'Positif']
label_predict_prob = model.predict(padded_test)
predicted_classes = np.argmax(label_predict_prob, axis=1)

threshold_positif = 0.5
threshold_netral = 0.5

predicted_labels = []

for prob in label_predict_prob:
    if prob[2] >= threshold_positif:
        predicted_labels.append('Positif')
    elif prob[1] >= threshold_netral:
        predicted_labels.append('Netral')
    else:
        predicted_labels.append('Negatif')

df_results = pd.DataFrame({
    'Text': text_test,
    'True_Label': [class_labels[np.argmax(label)] for label in label_test],
    'Predicted_Label': predicted_labels,
    'Probability': [max(prob) for prob in label_predict_prob]
})

df_results.head(50)